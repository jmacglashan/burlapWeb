<html>
	<head>
		<title>Basic Planning and Learning</title>
		<script src="../../google-code-prettify/run_prettify.js?skin=desert"></script>
		<link href="../../style.css" rel="stylesheet" type="text/css" media="screen" />
	</head>
	<body>
	
		<div id="nav">
			<span id="nav-content">
			<a href="../../index.html">BURLAP</a>
			<span id="nav-items"><a href="../../index.html">Home</a> | <a href="../../updates.html">Updates</a> | <a href="../../information.html">Information</a> | <a href="../../faq.html">F.A.Q.</a> | <a href="../index.html">Tutorials</a> | <a href="../../doc/index.html">Java Doc</a></span>
			</span>
		</div>
		<div id="page">
			<div id="page-content">
				<h1>Tutorial: Basic Planning and Learning</h1>
				<h2><a href="../index.html">Tutorials</a> > <a href="p1.html">Basic Planning and Learning</a> > Part 3</h2>
				<div id="index">
					<b>Tutorial Contents</b>
					<ul>
						<li><a href="p1.html#intro">Introduction</a></li>
						<li><a href="p1.html#shell">Creating the class shell</a></li>
						<li><a href="p2.html#init">Initializing the data members</a></li>
						<li><a href="p2.html#vis">Setting up a result visualizer</a></li>
						<li><a href="p3.html#bfs"><b>Planning with BFS</b></a></li>
						<li><a href="p3.html#dfs">Planning with DFS</a></li>
						<li><a href="p3.html#astar">Planning with A*</a></li>
						<li><a href="p4.html#vi">Planning with Value Iteration</a></li>
						<li><a href="p4.html#qlearn">Learning with Q-Learning</a></li>
						<li><a href="p4.html#sarsa">Learning with Sarsa(&lambda;)</a></li>
						<li><a href="p5.html#liveVis">Live Visualization</a></li>
						<li><a href="p5.html#vfvis">Value Function and Policy Visualization</a></li>
						<li><a href="p6.html#expPlot">Experimenter Tools and Performance Plotting</a></li>
						<li><a href="p6.html#conclusion">Conclusion</a></li>
						
					</ul>
				</div>
				<div id="sectionControls">
					<a href="p2.html">Previous Part</a> | <a href="p4.html">Next Part</a>
				</div>
				
				<h3><a name="bfs">Planning with BFS</a></h3>
				<p>
				One of the most basic deterministic planning algorithms is breadth-first search, which we will demonstrate first. Since we will
				be testing a number of different planning and learning algorithms in this tutorial, we will define a separate method
				for using each planning or learning algorithm and then you can simply select which one you want to try in the main
				method of the class. We will also pass each of these methods the output path to record its
				results. Lets start by defining the BFS method.</p>
				
				<pre class="prettyprint lang-java">
public void BFSExample(String outputPath){
		
	DeterministicPlanner planner = new BFS(domain, goalCondition, hashingFactory);
	Policy p = planner.planFromState(initialState);
	PolicyUtils.rollout(p, initialState, domain.getModel()).write(outputPath + "bfs");
		
}				
				</pre>
				<p>
				The first part of the method creates an instance of the <a href="../../doc/burlap/behavior/singleagent/planning/deterministic/uninformed/bfs/BFS.html">BFS</a> planning algorithm, which
				itself is a subclass of the <a href="../../doc/burlap/behavior/singleagent/planning/deterministic/DeterministicPlanner.html">DeterministicPlanner</a> class. To instantiate BFS, it only requires a reference to the domain,
				the goal condition for which it should search, and the HashableStateFactory. Because BFS implements the <a href="../../doc/burlap/behavior/singleagent/planning/Planner.html">Planner</a> interface, planning can be initiated by calling
				the <b>planFromState</b> method and passing it the initial state form which it should plan. The planFromState method automatically returns a <a href="../../doc/burlap/behavior/policy/Policy.html">Policy</a> object. Using the PolicyUtils class, we rollout the policy from an initial state. This method requires the model of the envionrment (or alternatively, you can have it rolled out within an actual environment). The result of the rollout method is an Episode object, which is a record of the state, action, and reward sequence from rolling out the policy. Episode objects can be written to disk, using the write method, which we call here.
				</p>

				<div class = "note">
					<h2>Using non-default policies</h2>
					<p>
					Each Planner implementation will return a different kind of Policy from planFromState that is relevant for the results the Planner stores. However, often times, after calling the planFromState method, you can wrap a different policy than the one that is returned around your planner instance to get slightly different behavior. For example, BFS's planFromState will return an <a href="../../doc/burlap/behavior/singleagent/planning/deterministic/SDPlannerPolicy.html">SDPlannerPolicy</a> instance, which will return the action the planner selected for any states on its solution path; if the policy is queried for a state not on the solution path, it will throw a runtime exception. However, you might choose to wrap a <a href="../../doc/burlap/behavior/singleagent/planning/deterministic/DDPlannerPolicy.html">DDPlannerPolicy</a> around BFS instead of using the returned SDPlannerPolicy. DDPlannerPolicy will act the same as SDPlannerPolicy except rather than throw a runtime exception if an action selection is queried for a state not on the current solution path, it will transparently recall planFromState on the new state to get an action to return; that is, it will perform replanning.</p>
				</div>
				<p>
				And that's all you need to code to plan with BFS on a defined domain and task!
				</p>
				<h3><a name="esvgui">Using the EpisodeSequenceVisualizer GUI </a></h3>
				<p>Now that the method to perform planning with BFS is defined, add a method call to it in your main method.</p>
				<pre class="prettyprint lang-java">
public static void main(String[] args) {


	BasicBehavior example = new BasicBehavior();
	String outputPath = "output/"; //directory to record results
	
	//run example
	example.BFSExample(outputPath);
	
	//run the visualizer
	example.visualize(outputPath);

}				
				</pre>
				<p>Note that our output path ended with a '/'. Whatever path you use, you should include the trailing '/' since the code we wrote to write the file will automatically append to that path name. 
				</p>
				<p>
				With the planning method hooked up, run the code! Because the task is simple, BFS should find a solution very
				quickly and print to the standard output the number of nodes it expanded in its search. Following that, the GUI
				should be launched for viewing the EpisodeAnalysis object that was recorded to a file. You should see something like the below
				image appear.
				</p><p>
				<center>
				<img src="images/visgui.png" width="600" />
				</center>
				</p><p>
				The main panel in the center of the window is used to render the current state selected. The text box at the
				bottom of the window will list all of the propositional functions that are true in that state, if the State is an OOState and the domain includes PropositionalFunction definitions. The leftmost
				list on the right side of the window lists all of the episode files that were recorded in the directory passed to the
				EpisodeSequenceVisualizer constructor. After selecting one of those instances, the list of actions taken in the
				episode are listed on the right-most list. Note that the action
				corresponds to the action that will be taken in the state
				that is currently visualized, so the result of the action
				will be seen in the next state. In the GridWorldDomain visualizer, the black cells represent walls, the grey circle
				represents the agent, and the blue cell represents the location object that we made.
				</p>
				
				
				<h3><a name="dfs">Planning with DFS</a></h3>
				<p>
				Another common search-based planning algorithm is depth-first search (DFS). Define the below method to provide
				a means to solve the task with DFS.
				</p>
				<pre class="prettyprint lang-java">
public void DFSExample(String outputPath){
		
	DeterministicPlanner planner = new DFS(domain, goalCondition, hashingFactory);
	Policy p = planner.planFromState(initialState);
	PolicyUtils.rollout(p, initialState, domain.getModel()).write(outputPath + "dfs");
	
}
				</pre>
				<p>
				You will notice that the code for DFS is effectively identical to the previous BFS code that we wrote, only this time
				the DeterministicPlanner is instantiated with a DFS object instead of a BFS object. DFS has a number of other constructors
				that allow you to specify other parameters such a depth limit, or maintaining a closed list. Feel free
				to experiment with them.
				</p><p>
				After you have defined the method, you can call it from the main method like we did the BFS method and visualize the
				results in the same way. Since DFS is not an optimal planner, it is likely that the solution it gives you will be
				much worse than the one BFS gave you!
				</p>
				
				<h3><a name="astar">Planning with A*</a></h3>
				<p>
				One of the most well known optimal search-based planning algorithms is A*. A* is an informed planner because it takes
				as input an <a href="http://en.wikipedia.org/wiki/Admissible_heuristic">admissible heuristic</a> 
				that estimates the cost to the goal from any given state. We can also use A* to plan a solution for our task
				as long as we also take the additional step of defining a heuristic to use (or you can use a <a href="../../doc/burlap/behavior/singleagent/planning/deterministic/informed/NullHeuristic.html">NullHeuristic</a> which will make A* uninformed). The below code defines a method
				for using A* with a Manhattan distance to goal heuristic.
				</p>
				
				<pre class="prettyprint lang-java">
public void AStarExample(String outputPath){
		
	Heuristic mdistHeuristic = new Heuristic() {

		public double h(State s) {
			GridAgent a = ((GridWorldState)s).agent;
			double mdist = Math.abs(a.x-10) + Math.abs(a.y-10);

			return -mdist;
		}
	};

	DeterministicPlanner planner = new AStar(domain, goalCondition, hashingFactory,
											 mdistHeuristic);

	Policy p = planner.planFromState(initialState);
	PolicyUtils.rollout(p, initialState, domain.getModel()).write(outputPath + "astar");
	
}				
				</pre>
				<p>
				To implement our heursitic, we made an annonmous class that implements Heuristic, which requires implementing the h method. We typecast the state to a GridWorldState, pull out the agent, and compute the Manhatten distance to 10, 10, our goal location. The h method then returns the negative value of the distance. We return the negative value, because BURLAP is based on <i>rewards</i> rather than <i>costs</i>. However, negative rewards are equivalent to costs, so the heursitic we give must be a non-positive value (<= 0). Note that A* also only operates on costs (negative rewards), so whenever using A*, you should make sure that the reward function for your domain returns negative values. By default, GridWorldDomain uses a UniformCostRF, which causes all rewards to be -1.
				</p>
				
				<p>
				Beyond defining a heuristic for A*, instantiating it and using it works mostly the same!
				</p>
				
				
				
				
				<div id="footerControls">
					<a href="p4.html">Next Part</a>
				</div>
				
			</div>
		</div>
		
	</body>
</html>
