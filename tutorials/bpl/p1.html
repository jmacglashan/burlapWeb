<html>
	<head>
		<title>Basic Planning and Learning</title>
		<script src="../../google-code-prettify/run_prettify.js?skin=desert"></script>
		<link href="../../style.css" rel="stylesheet" type="text/css" media="screen" />
	</head>
	<body>
	
		<div id="nav">
			<span id="nav-content">
			<a href="../../index.html">BURLAP</a>
			<span id="nav-items"><a href="../../index.html">Home</a> | <a href="../../updates.html">Updates</a> | <a href="../../information.html">Information</a> | <a href="../../faq.html">F.A.Q.</a> | <a href="../index.html">Tutorials</a> | <a href="../../doc/index.html">Java Doc</a></span>
			</span>
		</div>
		<div id="page">
			<div id="page-content">
				<h1>Tutorial: Basic Planning and Learning</h1>
				<h2><a href="../index.html">Tutorials</a> > <a href="p1.html">Basic Planning and Learning</a> > Part 1</h2>
				<div id="index">
					<b>Tutorial Contents</b>
					<ul>
						<li><a href="p1.html#intro"><b>Introduction</b></a></li>
						<li><a href="p1.html#shell">Creating the class shell</a></li>
						<li><a href="p2.html#init">Initializing the data members</a></li>
						<li><a href="p2.html#vis">Setting up a result visualizer</a></li>
						<li><a href="p3.html#bfs">Planning with BFS</a></li>
						<li><a href="p3.html#dfs">Planning with DFS</a></li>
						<li><a href="p3.html#astar">Planning with A*</a></li>
						<li><a href="p4.html#vi">Planning with Value Iteration</a></li>
						<li><a href="p4.html#qlearn">Learning with Q-Learning</a></li>
						<li><a href="p4.html#sarsa">Learning with Sarsa(&lambda;)</a></li>
						<li><a href="p5.html#liveVis">Live Visualization</a></li>
						<li><a href="p5.html#vfvis">Value Function and Policy Visualization</a></li>
						<li><a href="p6.html#expPlot">Experimenter Tools and Performance Plotting</a></li>
						<li><a href="p6.html#conclusion">Conclusion</a></li>
						
					</ul>
				</div>
				<div id="sectionControls">
					<a href="p2.html">Next Part</a>
				</div>
				
				<br/><br/>
				<b>You are viewing the tutorial for BURLAP 3; if you'd like the BURLAP 2 tutorial, go <a href="../../tutorials_v1/bpl/p1.html">here</a>.</b>

				<h3><a name="intro">Introduction</a></h3>
				<p>
				The purpose of this tutorial is to get you familiar with using some of the planning and learning algorithms
				in BURLAP. Specifically, this tutorial will cover instantiating a grid world domain bundled with BURLAP, and having the task solved with Q-learning, Sarsa learning, BFS, DFS, A*, and Value
				Iteration. This tutorial will also show you how to visualize these results in various ways using 
				tools in BURLAP. The take home message you should get from this tutorial is that using different planning
				and learning algorithms largely amounts to changing the algorithm Java object you instantiate, with everything
				else being the same. You are encouraged to extend this tutorial on your own using some of the other planning
				and learning algorithms in BURLAP.
				</p>
				<p>
				The complete set of code written in this tutorial is availabe at the end, and in the <a href="https://github.com/jmacglashan/burlap_examples/">burlap_examples</a> github repository.
				</p>
				<h3><a name="shell">Creating the class shell</a></h3>
				<p>
				For this tutorial, we will start by making a class that has data members for all the domain and task relevant
				properties. In the tutorial we will call this class "BasicBehavior" but feel free to name it to whatever you like.
				Since we will also be running the examples from this class, we'll include a main method. For convenience, we have also included at the start all of the class the imports that you will need for this tutorial. If you have a good IDE, like IntelliJ or Eclipse, those can 
				auto import the classes as you go so that you never have to write an import line yourself.
				</p>
				<pre class="prettyprint lang-java">	
import burlap.behavior.policy.GreedyQPolicy;
import burlap.behavior.policy.Policy;
import burlap.behavior.policy.PolicyUtils;
import burlap.behavior.singleagent.Episode;
import burlap.behavior.singleagent.auxiliary.EpisodeSequenceVisualizer;
import burlap.behavior.singleagent.auxiliary.StateReachability;
import burlap.behavior.singleagent.auxiliary.performance.LearningAlgorithmExperimenter;
import burlap.behavior.singleagent.auxiliary.performance.PerformanceMetric;
import burlap.behavior.singleagent.auxiliary.performance.TrialMode;
import burlap.behavior.singleagent.auxiliary.valuefunctionvis.ValueFunctionVisualizerGUI;
import burlap.behavior.singleagent.auxiliary.valuefunctionvis.common.ArrowActionGlyph;
import burlap.behavior.singleagent.auxiliary.valuefunctionvis.common.LandmarkColorBlendInterpolation;
import burlap.behavior.singleagent.auxiliary.valuefunctionvis.common.PolicyGlyphPainter2D;
import burlap.behavior.singleagent.auxiliary.valuefunctionvis.common.StateValuePainter2D;
import burlap.behavior.singleagent.learning.LearningAgent;
import burlap.behavior.singleagent.learning.LearningAgentFactory;
import burlap.behavior.singleagent.learning.tdmethods.QLearning;
import burlap.behavior.singleagent.learning.tdmethods.SarsaLam;
import burlap.behavior.singleagent.planning.Planner;
import burlap.behavior.singleagent.planning.deterministic.DeterministicPlanner;
import burlap.behavior.singleagent.planning.deterministic.informed.Heuristic;
import burlap.behavior.singleagent.planning.deterministic.informed.astar.AStar;
import burlap.behavior.singleagent.planning.deterministic.uninformed.bfs.BFS;
import burlap.behavior.singleagent.planning.deterministic.uninformed.dfs.DFS;
import burlap.behavior.singleagent.planning.stochastic.valueiteration.ValueIteration;
import burlap.behavior.valuefunction.QProvider;
import burlap.behavior.valuefunction.ValueFunction;
import burlap.domain.singleagent.gridworld.GridWorldDomain;
import burlap.domain.singleagent.gridworld.GridWorldTerminalFunction;
import burlap.domain.singleagent.gridworld.GridWorldVisualizer;
import burlap.domain.singleagent.gridworld.state.GridAgent;
import burlap.domain.singleagent.gridworld.state.GridLocation;
import burlap.domain.singleagent.gridworld.state.GridWorldState;
import burlap.mdp.auxiliary.stateconditiontest.StateConditionTest;
import burlap.mdp.auxiliary.stateconditiontest.TFGoalCondition;
import burlap.mdp.core.TerminalFunction;
import burlap.mdp.core.state.State;
import burlap.mdp.core.state.vardomain.VariableDomain;
import burlap.mdp.singleagent.common.GoalBasedRF;
import burlap.mdp.singleagent.common.VisualActionObserver;
import burlap.mdp.singleagent.environment.SimulatedEnvironment;
import burlap.mdp.singleagent.model.FactoredModel;
import burlap.mdp.singleagent.oo.OOSADomain;
import burlap.statehashing.HashableStateFactory;
import burlap.statehashing.simple.SimpleHashableStateFactory;
import burlap.visualizer.Visualizer;

import java.awt.*;
import java.util.List;


public class BasicBehavior {

	
	
	GridWorldDomain gwdg;
	OOSADomain domain;
	RewardFunction rf;
	TerminalFunction tf;
	StateConditionTest goalCondition;
	State initialState;
	HashableStateFactory hashingFactory;
	SimulatedEnvironment env;
	
	
	public static void main(String[] args) {
	
		//we'll fill this in later
	
	}
	
	
}
				</pre>
				<p>
				If you're already familiar with MDPs in general, the importance of some of these data members will be
				obvious. However, we will walk through in detail what each data member is and why we're going to need it.
				</p><p class="list">
				<b>GridWorldDomain	gwdg</b><br/> A <a href="../../doc/burlap/domain/singleagent/gridworld/GridWorldDomain.html">GridWorldDomain</a> is a <a href="../../doc/burlap/mdp/auxiliary/DomainGenerator.html">DomainGenerator</a> implementation for creating grid worlds.
				</p><p class="list">
				<b>Domain domain</b><br/> An <a href="../../doc/burlap/mdp/singleagent/oo/OOSADomain.html">OOSADomain</a> object is a fundamental class for defining problem domains that have OO-MDP state representations (although we will not focus on the OO-MDP aspects here). In short, any Domain object contain all of the elements of an MDP, except the entire state space, which is not included since for many domains it may be infinite.
				</p>
				</p><p class="list">
				<b>TerminalFunction tf</b><br/> By default, our grid world will use a <a href="../../doc/burlap/mdp/singleagent/common/UniformCostRF.html">UniformCostRF</a>, a reward function that returns -1 everywhere. But if we want to specify a goal state, we need to tell our GridWorld generator which states are terminal states, which we do with a TerminalFunction.
				<a href="../../doc/burlap/mdp/core/TerminalFunction.html">TerminalFunction</a> is an interface with a boolean method that defines which states are terminal states.
				</p><p class="list">
				<b>StateConditionTest goalCondition</b><br/> Not all planning algorithms are designed to maximize reward functions. Many
				are instead defined as search algorithms that seek action sequences that will cause the agent to reach specific goal states. 
				A <a href="../../doc/burlap/mdp/auxiliary/stateconditiontest/StateConditionTest.html">StateConditionTest</a> is an interface with a boolean method that takes a state as an argument similar to a
				TerminalFunction, only we use it as a means to specify arbitrary state conditions, rather than just terminal states. We will use StateConditionTest to specify the goal state(s) of search-based planning algorithms.
				</p><p class="list">
				<b>State initialState</b><br/> Since domains are not required to enumerate entire state spaces, we will need to define at least the initial state of our problem, which we hold in this data member.
				<p/>
				
				
				<p class="list">
				<b>HashableStateFactory hasingFactory</b><br/> In this tutorial we will cover tabular algorithms; algorithms that learn or plan with tabular identifiers for states (in the later <a href="../scd/p1.html">Solving Continuous Domains tutorial</a>, we will cover how to use BURLAP to solve continuous domains). Typically, for fast access, tabular algorithms will associate values for states in a HashMap, which means tabular methods need some way to compute hash codes and test equality of states. The obvious solution is for State implementations to implement the Java equals and hashCode methods. However, it is not uncommon that differnet scenarios will require different ways of computing hash codes or state equality that the creator of the State did not anticipate, such as state abstraction or variable discretization. Therefore, BURLAP makes use of <a href="../../doc/burlap/statehashing/HashableStateFactory.html">HashableStateFactory</a> objects that allows a client to specify how to hash and check state equality for states. There are a number of default implementations also provided in BURLAP.
				</p>


				<p class="list">
				<b>Environment env</b><br/>
				Learning algorithms address a problem in which the agent observes its environment, makes a decision, and then observes how the environment changes. This is a challenging problem because initially, an agent will not know how the environment works or what a good decision is, but must live with the consequences of their decision. To facilitate the construction of learning problems, all single-agent learning algorithms in BURLAP (algorithms that implement the <a href="../../doc/burlap/behavior/singleagent/learning/LearningAgent.html">LearningAgent</a> interface), interact with an implementation of the <a href="../../doc/burlap/mdp/singleagent/environment/Environment.html">Environment</a> interface. One of the included concrete implementations is <a href="../../doc/burlap/mdp/singleagent/environment/SimulatedEnvironment.html">SimulatedEnvironment</a>, which you can use when you're constructing an Environment for a BURLAP domain that has an included model. Other concrete Environment implementations in BURLAP or library extensions to BURLAP include
				</p>
				<ul>
				<li>RL Glue hooks using a BURLAP agent with an RL Glue Environment.</li>
				<li><a href="https://github.com/h2r/burlap_rosbridge">burlap_rosbridge</a> for using BURLAP with robots embodied in the real world.</li>
				<li><a href="https://github.com/h2r/burlapcraft">BurlapCraft</a>, for using BURLAP to control a Minecraft player.</li>
				</ul> 
				<p>
				Since Environment is an interface, you can also easily implement your own version if you need a BURLAP agent to interact with external code or systems that are not already provided in BURLAP.
				</p>
				<div id="footerControls">
					<a href="p2.html">Next Part</a>
				</div>
				
			</div>
		</div>
		
	</body>
</html>
