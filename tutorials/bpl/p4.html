<html>
	<head>
		<title>Basic Planning and Learning</title>
		<script src="../../google-code-prettify/run_prettify.js?skin=desert"></script>
		<link href="../../style.css" rel="stylesheet" type="text/css" media="screen" />
	</head>
	<body>
	
		<div id="nav">
			<span id="nav-content">
			<a href="../../index.html">BURLAP</a>
			<span id="nav-items"><a href="../../index.html">Home</a> | <a href="../../updates.html">Updates</a> | <a href="../../information.html">Information</a> | <a href="../../faq.html">F.A.Q.</a> | <a href="../index.html">Tutorials</a> | <a href="../../doc/index.html">Java Doc</a></span>
			</span>
		</div>
		<div id="page">
			<div id="page-content">
				<h1>Tutorial: Basic Planning and Learning</h1>
				<h2><a href="../index.html">Tutorials</a> > <a href="p1.html">Basic Planning and Learning</a> > Part 4</h2>
				<div id="index">
					<b>Tutorial Contents</b>
					<ul>
						<li><a href="p1.html#intro">Introduction</a></li>
						<li><a href="p1.html#shell">Creating the class shell</a></li>
						<li><a href="p2.html#init">Initializing the data members</a></li>
						<li><a href="p2.html#vis">Setting up a result visualizer</a></li>
						<li><a href="p3.html#bfs">Planning with BFS</a></li>
						<li><a href="p3.html#dfs">Planning with DFS</a></li>
						<li><a href="p3.html#astar">Planning with A*</a></li>
						<li><a href="p4.html#vi"><b>Planning with Value Iteration</b></a></li>
						<li><a href="p4.html#qlearn">Learning with Q-Learning</a></li>
						<li><a href="p4.html#sarsa">Learning with Sarsa(&lambda;)</a></li>
						<li><a href="p5.html#liveVis">Live Visualization</a></li>
						<li><a href="p5.html#vfvis">Value Function and Policy Visualization</a></li>
						<li><a href="p6.html#expPlot">Experimenter Tools and Performance Plotting</a></li>
						<li><a href="p6.html#conclusion">Conclusion</a></li>
						
					</ul>
				</div>
				<div id="sectionControls">
					<a href="p3.html">Previous Part</a> | <a href="p5.html">Next Part</a>
				</div>
				
				<h3><a name="vi">Planning with Value Iteration</a></h3>
				<p>
				A common stochastic domain planner is Value Iteration (VI). An advantage of VI is that it will compute the
				policy for the entire state space that is reachable from the initial state that
				is passed to the planFromState method. To set up a VI planner, define the following method.
				</p>
				<pre class="prettyprint lang-java">
public void valueIterationExample(String outputPath){
		
	Planner planner = new ValueIteration(domain, 0.99, hashingFactory, 0.001, 100);
	Policy p = planner.planFromState(initialState);

	PolicyUtils.rollout(p, initialState, domain.getModel()).write(outputPath + "vi");
	
}
				</pre>
				<p>
				Instantiating value iteration works a lot like our deterministic search planning algorithms. However, because Value Iteration is based on solving MDPs, it requires us to specify a discount factor to use; we've chosen 0.99. It also needs stopping criteria specified, because Value Iteration, as the name implies, is an interative algorithm and we need to define when enough iterations for a good solutions have been performed. We've chossen for VI to terminate when either the changes in the value function are no longer than 0.001, or 100 iterations over the state space have been performed.
				</p><p>
				VI computes the optimal value function for the problem, which specifies the expected future discounted reward for taking each action in each state (the Q-function). Therefore, it returns a <a href="../../doc/burlap/behavior/policy/GreedyQPolicy.html">GreedyQPolicy</a>. This policy looks at the Q-values the planner computes and returns the action with the maximium Q-value (and breaks ties randomly). This policy can be used with any planning or learning algorithm that returns Q-values by implementing the <a href="../../doc/burlap/behavior/valuefunction/QProvider.html">QProvider</a> interface.
				</p><p>
				Try
				setting the main method to call our newly defined VI example method now.
				</p>
				
				
				<h3><a name="qlearn">Learning with Q-Learning</a></h3>
				<p>
				All of the previous examples were examples of using planning algorithms to solve our task. In this section,
				we will diverge from that and use a learning algorithm, Q-learning, to solve the task. Ultimately, learning
				algorithms are utilized in much the same way as planning algorithms, except you will run multiple
				episodes of learning in which the agent interacts with an Environment instance to solve it (or one very long episode if it is a continuing task rather than an episodic
				task). The method you should define to utilize Q-learning is shown below.
				</p>
				<pre class="prettyprint lang-java">
public void QLearningExample(String outputPath){
		
	LearningAgent agent = new QLearning(domain, 0.99, hashingFactory, 0., 1.);

	//run learning for 50 episodes
	for(int i = 0; i < 50; i++){
		Episode e = agent.runLearningEpisode(env);

		e.write(outputPath + "ql_" + i);
		System.out.println(i + ": " + e.maxTimeStep());

		//reset environment for next learning episode
		env.resetEnvironment();
	}
	
}				
				</pre>
				<p>
				Lets first look at the constructor. Rather than a planning instance, we're creating a <a href="../../doc/burlap/behavior/singleagent/learning/LearningAgent.html">LearningAgent</a>
				instance which provides some methods for learning with an environment. <a href="../../doc/burlap/behavior/singleagent/learning/tdmethods/QLearning.html">QLearning</a> is an instance of the LearningAgent interface
				and takes parameters for the domain, a discount factor, a HashableStateFactory, an initial value for the Q-values, and a learning rate (which for a deterministic domain, 1.0 is a good choice). Note that this
				constructor will by default set Q-learning to use a 0.1 epsilon greedy policy. There are other constructors
				that allow you to set which learning policy to use and there is also a setter that allows you to set it
				if you'd like to use a different policy. Other parameters for Q-learning could also be set, but we will not detail them here.
				</p>
				<p>
				With the QLearning instance created, next we will run 50 learning episodes, so we set up a for loop.
				To run a learning episode, we call the method <b>runLearningEpisode</b> on the LearningAgent instance
				and pass it the Environment in which learning will be performed. The method  returns an Episode object (similar to policies) so that a record of the interactions can be examined. As before, we can then write the returned episode to disk for viewing later.
				</p><p>
				Finally, at the end of the loop, we call the <b>resetEnvironment</b> method on the Environment. This method is the typical way to signal that an Environment needs to reset to an initial state from its current state, which may be a terminal state. When the method returns, it is expected that the environment in a non-terminal state from which an agent can act again.
				</p><p>
				After that, you can call this method from your main method and run the agents behavior for each of the 50 episodes of learning! You should find that as learning progessed, the agent got better. By the end, the agent's behavior will still be slightly random since it's following an epislon greedy policy that always takes some random actions. However, since QLearning implements the QFunction interface, you could always wrap a GreedyQPolicy around it, like with VI, to remove random action selection.
				</p>
				
				
				
				<h3><a name="sarsa">Learning with Sarsa(&lambda;)</a></h3>
				<p>
				A similar learning algorithm to Q-learning is Sarsa(&lambda;). The first difference between the two
				algorithms is that Sarsa(&lambda;) updates Q-values with respect to the Q-value of the next action taken,
				rather than the maximum Q-value of the next state (see <a href="http://en.wikipedia.org/wiki/SARSA">Wikipedia</a> 
				for more information). The second, and larger, difference is that at every time step, Sarsa(&lambda;) 
				will also update the Q-values
				for state-action pairs experienced previously in an episode with respect to 
				the amount specified by &lambda; and how long ago the experiences occurred. Define the below method to solve our
				task with Sarsa(&lambda;).
				</p>
				<pre class="prettyprint lang-java">
public void SarsaLearningExample(String outputPath){
		
	LearningAgent agent = new SarsaLam(domain, 0.99, hashingFactory, 0., 0.5, 0.3);

	//run learning for 50 episodes
	for(int i = 0; i < 50; i++){
		Episode e = agent.runLearningEpisode(env);

		e.write(outputPath + "sarsa_" + i);
		System.out.println(i + ": " + e.maxTimeStep());

		//reset environment for next learning episode
		env.resetEnvironment();
	}
	
}				
				</pre>
				<p>
				You will notice that this method looks pretty identical to the Q-learning example, except this time
				a <a href="../../doc/burlap/behavior/singleagent/learning/tdmethods/SarsaLam.html">SarsaLam</a> instance is constructed. Additionally, we lowered the learning rate to 0.5 (typically
				you should use lower learning rates when you have a higher value of &lambda;). The last parameter of
				the constructor is the &lambda; value which we set to 0.3. A value of &lambda;=1.0 effectively makes algorithm run an
				online Monte Carlo in which the effects of all future interactions are fully considered in updating each Q-value
				of an episode. It is not always a good idea to use a large &lambda; value.
				</p><p>
				Otherwise, the rest is the same; you can call this method from the main method and give it shot!
				</p>
				
				
				
				
				<div id="footerControls">
					<a href="p5.html">Next Part</a>
				</div>
				
			</div>
		</div>
		
	</body>
</html>
