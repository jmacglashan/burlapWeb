<html>
	<head>
		<title>BURLAP</title>
		<script src="google-code-prettify/run_prettify.js?skin=desert"></script>
		<link href="style.css" rel="stylesheet" type="text/css" media="screen" />
	</head>
	<body>
	
		<div id="nav">
			<span id="nav-content">
			<a href="index.html">BURLAP</a>
			<span id="nav-items"><a href="index.html">Home</a> | <a href="updates.html">Updates</a> | <a href="information.html">Information</a> | <a href="faq.html">F.A.Q.</a> | <a href="tutorials/index.html">Tutorials</a> | <a href="doc/index.html">Java Doc</a></span>
			</span>
		</div>
		<div id="page">
			<div id="page-content">
				<center>
				<img src="images/blocksWorldImage.png" height="150" style="margin-right:6em;"/> <img src="images/gwvf.png" height="150" style="margin-right:6em;"/> <img src="images/mountainCarImage.png" height="150" />
				</center>
				<br/><br/>
				<h1>About</h1>
				<p>
				The Brown-UMBC Reinforcement Learning and Planning (<b>BURLAP</b>) java code library is for the use and 
				development of single or multi-agent planning and 
				learning algorithms and domains to accompany them. At the core of the library is a rich state 
				and domain representation framework based on the object-oriented MDP (OO-MDP) [1] paradigm that 
				facilitates the creation of discrete, continuous, or relational domains that can consist of any 
				number of different "objects" in the world. Planning and learning algorithms range from classic 
				forward search planning to value function-based stochastic planning and learning algorithms. 
				Also included is a set of analysis tools such as a common framework for the visualization of 
				domains and agent performance in various domains.
				</p>
				<p>
				BURLAP is licensed under LGPL so that it can be used freely in any number of circumstances, while remaining open source.
				</p>
				<p>
				For more background information on the project and the people involved, see the <a href="information.html">Information</a> page. 
				</p>
				<p>
				
				
				
				<h1>Where to <i>git</i> it</h1>
				<p>
				BURALP now fully supports Maven and is available on Maven Central! That means that if you'd like to create a project that uses BURLAP, all you need to do is add the following dependency to the &lt;dependencies&gt; section of your projects pom.xml
				<xmp style="background-color: #333333; color: white; padding-left: 1em">
<dependency>
  <groupId>edu.brown.cs.burlap</groupId>
  <artifactId>burlap</artifactId>
  <version>2.1.1</version>
</dependency>
				</xmp>
				and the library will automatically be downloaded and linked to your project! If you do not have Maven installed, you can get it from <a href="https://maven.apache.org/download.cgi">here</a>.
				<p>
				You can also get thefull BURLAP source to manually compile/modify from Github at:<br/>
				<a href="https://github.com/jmacglashan/burlap" target="external">https://github.com/jmacglashan/burlap</a>
				</p>
				<p>
				Alternatively, you can directly download precompiled jars from Maven Central from <a href="http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22edu.brown.cs.burlap%22%20AND%20a%3A%22burlap%22">here</a>. Use the jar-with-dependencies if you want all dependencies included.
				</p>
				<p>
				If you are looking for the older version 1 of BURLAP, you can get the pre-compiled jars for it below, or use the v1 branch on git.
				</p>
				<ul>
				<li><a href="burlap_v1.jar">version 1 pre-compiled jar</a> with dependencies included, or</li>
				<li><a href="burlap_v1_no_dep.jar">version 1 pre-compiled jar</a> with<b>out</b> dependencies.</li>
				</ul>
				<p>
				The Java doc for version 1 can also be found <a href="doc_v1/index.html">here</a>.
				</p>
				
				<h1>Tutorials and Example Code</h1>
				<p>
				Short video tutorials, longer text tutorials, and example code are available for BURLAP.
				All code can be found in our examples repository, which also provides the kind of POM file and file sturcture you should consider using for a BURLAP project. The example repository can be found at:
				</p>
				<p>
				<a href="https://github.com/jmacglashan/burlap_examples/"><b>https://github.com/jmacglashan/burlap_examples/</b></a>
				</p>

				<h2>Video Tutorials</h2>
				<iframe width="560" height="315" src="https://www.youtube.com/embed/PcP3DkMQ5_M" frameborder="0" allowfullscreen></iframe>
				<h2>Written Tutorials</h2>
				<ul>
				<li><a href="tutorials/hgw/p1.html">Hello <i>Grid</i>World!</a> - a tutorial on acquiring and linking with BURLAP</li>
				<li><a href="tutorials/bd/p1.html">Building a Domain</a></li>
				<li><a href="tutorials/bpl/p1.html">Using Basic Planning and Learning Algorithms</a></li>
				<li><a href="tutorials/cpl/p1.html">Creating a Planning and Learning Algorithm</a></li>
				<li><a href="tutorials/scd/p1.html">Solving Continuous Domains</a></li>
				</ul>
	
			
				<h1>Documentation</h1>
				<p>
				Java documentation is provided for all of the source code in BURLAP. 
				You can find an online copy of the javadoc at the below location.
				</p>
				<p>
				<a href="http://burlap.cs.brown.edu/doc/index.html">http://burlap.cs.brown.edu/doc/index.html</a>
				</p>
				<h1>Features</h1>
				<h2>Current</h2>
				<p>
				<ul>
					<li>
						Problem Representation using the OO-MDP paradigm which represents states as collections of objects, each represented
						by their own feature vector. Domains may also be defined with propositional functions that operate on objects in the world
						that are useful for defining and learning reward functions and transition dynamics.
						The OO-MDP formalism supports
						<ul>
							<li>Discrete attributes</li>
							<li>Continuous attributes</li>
							<li>Relational attributes (that link to either one or many other objects in the world)</li>
							<li>Parameterized actions (e.g., actions 
							that operate on objects in the world)</li>
							<li>Actions with preconditions</li>
							<li>Actions that dynamically create new objects</li>
						</ul>
					</li>
					<li>
						Supported problem formalisms
						<ul>
							<li>Markov Decision Processes (single agent)</li>
							<li>Stochastic Games (multi-agent)</li>
							<li>Partially Observable Markov Decision Processes (single agent)</li>
						</ul>
					</li>
					<li>Tools for visualizing and defining visualizations of states, episodes, value functions, and policies.</li>
					<li>Tools for setting up experiments with multiple learning algorithms and plotting the performance using multiple performance metrics.</li>
					<li>An extendable shell framework for controlling experiments at runtime.</li>
					<li>Tools for creating multi-agent tournaments.</li>
					<li>
						Classic goal-directed deterministic forward-search planning.
						<ul>
							<li>Breadth-first Search</li>
							<li>Depth-first Search</li>
							<li>A*</li>
							<li>IDA* </li>
							<li>Statically Weighted A* [2]</li>
							<li> Dynamically Weighted A* [3]</li>
						</ul>
					</li>
					<li>
						Stochastic Planning.
						<ul>
							<li>Value Iteration [4]</li>
							<li>Policy Iteration</li>
							<li>Prioritized Sweeping [20]</li>
							<li>Real-time Dynamic Programming [5]</li>
							<li>UCT [6]</li>
							<li>Sparse Sampling [17]</li>
							<li>Bounded Real-time Dynamic Programming [21]</li>
						</ul>
					</li>
					<li>
						Learning.
						<ul>
							<li>Q-learning [7]</li>
							<li>SARSA(&lambda;) [8]</li>
							<li>Actor Critic [9]</li>
							<li>Potential Shaped RMax [12]</li>
							<li>ARTDP [5]</li>
						</ul>
					</li>
					<li>
						Value Function Approximation
						<ul>
							<li>Gradient Descent SARSA(&lambda;) [8]</li>
							<li>Least-Squares Policy Iteration [18]</li>
							<li>Fitted Value Iteration [24]</li>
							<li>Framework for implementing linear and non-linear VFA</li>
							<li>CMACs/Tile Coding [10]</li>
							<li>Radial Basis Functions</li>
							<li>Fourier Basis Functions [19]</li>
						</ul>
					</li>
					<li>The Options framework [11] (supported in nearly all current single agent planning and learning algorithms).
					<li>Reward Shaping</li>
					<li>Inverse Reinforcement Learning
						<ul>
							<li>Maximum Margin Apprenticeship Learning [16]</li>
							<li>Multiple Intentions Maximum-likelihood Inverse Reinforcement Learning [22]</li>
							<li>Receding Horizon Inverse Reinforcement Learning [23]</li>
						</ul>
					</li>
					<li>Multi-agent Q-learning and Value Iteration, supporting
						<ul>
						<li>Q-learning with an n-step action history memory</li>
						<li>Friend-Q [13]</li>
						<li>Foe-Q [13] </li>
						<li>Correlated-Q [14]</li>
						<li>Coco-Q [15]</li>
						</ul>
					</li>
					<li>Single-agent partially observable planning algorithms
						<ul>
							<li>Finite horizon optimal tree search</li>
							<li>QMDP [25]</li>
							<li>Belief MDP conversion for use with standard MDP algorithms</li>
						</ul>
					</li>
					<li>
						Pre-made domains and domain generators.
						<ul>
							<li>Grid Worlds</li>
							<li>Domains represented as graphs</li>
							<li>Blocks World</li>
							<li>Lunar Lander</li>
							<li>Mountain Car</li>
							<li>Cart Pole</li>
							<li>Frostbite</li>
							<li>Blockdude</li>
							<li>Grid Games (a multi-agent stochastic games domain)</li>
							<li>Multiple classic Bimatrix games.
						</ul>
					</li>
					<li><a href="http://glue.rl-community.org/wiki/Main_Page" target="external">RLGlue</a> agent and environment interfacing</li>
					<li><a href="https://github.com/h2r/burlap_rosbridge" target="external">Extensions</a> for controlling <a href="http://www.ros.org/" target="external">ROS</a>-powered robots</li>
					<li><a href="https://github.com/h2r/burlapcraft" target="external">Extensions</a> for controlling <a href="http://www.minecraft.net" target="external">Minecraft</a></li>
				</ul>
				</p>
				<h2>Features in development</h2>
				<ul>
					<li>Learning from human feedback algorithms</li>
					<li>POMDP algorithms like POMCP and PBVI</li>
					<li>General growth of all other algorithm classes already included</li>
				</ul>
				
				<h1>References</h1>
				<p>
				<ol>
					<li>Diuk, C., Cohen, A., and Littman, M.L.. "An object-oriented representation for efficient reinforcement learning." Proceedings of the 25th international conference on Machine learning (2008). 240-270.</li>
					<li>Pohl, Ira. "First results on the effect of error in heuristic search". Machine Intelligence 5 (1970): 219-236.</li>
					<li>Pohl, Ira. "The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic weighting and computational issues in heuristic problem solving (August, 1973)</li>
					<li>Puterman, Martin L., and Moon Chirl Shin. "Modified policy iteration algorithms for discounted Markov decision problems." Management Science 24.11 (1978): 1127-1137.</li>
					<li>Barto, Andrew G., Steven J. Bradtke, and Satinder P. Singh. "Learning to act using real-time dynamic programming." Artificial Intelligence 72.1 (1995): 81-138.</li>
					<li>Kocsis, Levente, and Csaba Szepesvari. "Bandit based monte-carlo planning." ECML (2006). 282-293.</li>
					<li>Watkins, Christopher JCH, and Peter Dayan. "Q-learning." Machine learning 8.3-4 (1992): 279-292.</li>
					<li>Rummery, Gavin A., and Mahesan Niranjan. On-line Q-learning using connectionist systems. University of Cambridge, Department of Engineering, 1994.</li>
					<li>Barto, Andrew G., Richard S. Sutton, and Charles W. Anderson. "Neuronlike adaptive elements that can solve difficult learning control problems." Systems, Man and Cybernetics, IEEE Transactions on 5 (1983): 834-846.</li>
					<li>Albus, James S. "A theory of cerebellar function." Mathematical Biosciences 10.1 (1971): 25-61.</li>
					<li>Sutton, Richard S., Doina Precup, and Satinder Singh. "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning." Artificial intelligence 112.1 (1999): 181-211.</li>
					<li>Asmuth, John, Michael L. Littman, and Robert Zinkov. "Potential-based Shaping in Model-based Reinforcement Learning." AAAI. 2008.</li>
					<li>Littman, Michael L. "Markov games as a framework for multi-agent reinforcement learning." ICML. Vol. 94. 1994.</li>
					<li>Greenwald, Amy, Keith Hall, and Roberto Serrano. "Correlated Q-learning." ICML. Vol. 3. 2003.</li>
					<li>Sodomka, Eric, Hilliard, E., Littman, M., & Greenwald, A. "Coco-Q: Learning in Stochastic Games with Side Payments." Proceedings of the 30th International Conference on Machine Learning (ICML-13). 2013.</li>
					<li>Abbeel, Pieter, and Andrew Y. Ng. "Apprenticeship learning via inverse reinforcement learning." Proceedings of the twenty-first international conference on Machine learning. ACM, 2004.</li>
					<li>Kearns, Michael, Yishay Mansour, and Andrew Y. Ng. "A sparse sampling algorithm for near-optimal planning in large Markov decision processes." Machine Learning 49.2-3 (2002): 193-208.</li>
					<li>Lagoudakis, Michail G., and Ronald Parr. "Least-squares policy iteration." The Journal of Machine Learning Research 4 (2003): 1107-1149</li>
					<li>G.D. Konidaris, S. Osentoski and P.S. Thomas. Value Function Approximation in Reinforcement Learning using the Fourier Basis. In Proceedings of the Twenty-Fifth Conference on Artificial Intelligence, pages 380-385, August 2011.</li>
					<li>Li, Lihong, Michael L. Littman, and L. Littman. Prioritized sweeping converges to the optimal value function. Tech. Rep. DCS-TR-631, 2008.</li>
					<li>McMahan, H. Brendan, Maxim Likhachev, and Geoffrey J. Gordon. "Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees."  Proceedings of the 22nd international conference on Machine learning. ACM, 2005.</li>
					<li>Babes, Monica, et al. "Apprenticeship learning about multiple intentions." Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.</li>
					<li>MacGlashan, James and Littman, Micahel, "Between imitation and intention learning," in Proceedings of the International Joint Conference on Artificial Intelligence, 2015.</li>
					<li>Gordon, Geoffrey J. "Stable function approximation in dynamic programming." Proceedings of the twelfth international conference on machine learning. 1995.</li>
					<li>Littman, M.L., Cassandra, A.R., Kaelbling, L.P., "Learning Policies for Partially Observable Environments: Scaling Up," in Proceedings of the 12th Internaltion Conference on Machine Learning. 1995.</li> 
				</ol>
				</p>
			</div>
		</div>
		
	</body>
</html>